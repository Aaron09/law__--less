arly back down to zero over the interval 0  u  1. SincefX u > 0 only for 1 < u < 1, the conditional density fY |X v|u is dened only for u in thatinterval. The cases 1 < u  0 and 0 < u < 1 will be considered separately, because fX has adierent form over those two intervals. So the conditional pdf of Y given X = u is1  u < 0  fY |X v|u =0  u < 1  fY |X v|u =cid:26 2cid:26 21+u01u01+u2  v  1 + u2  v  1else1+uelse uniform on uniform oncid:21cid:20 1 + ucid:20 1 + u2, 1 + ucid:21, 1,2As indicated, the distribution of Y given X = u for u xed, is the uniform distribution over aninterval depending on u, as long as 1 < u < 1. The mean of a uniform distribution over an intervalis the midpoint of the interval, and thus:gu = EY |X = u = 31+u3+u44undened else.1 < u  00 < u < 1This estimator is shown in Figure 4.24b. Note that this estimator could have been drawn byinspection. For each value of u in the interval 1 < u < 1, EY |X = u is just the center of massof the cross section of the support of fX,Y along the vertical line determined by u. That is truewhenever X, Y  is uniformly distributed over some region. The mean square error given X = uis the variance of a uniform distribution on an interval of length 1|u|. Averaging, which is 1|u|2248Xab01uvEY|X=u10cid:2391uf  ucid:23911EY|X=u210CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESover u using the pdf fX yields thatMSE for gX =cid:90 0cid:90 111 + udu +1  |u|2481  |u|2cid:90 101  u1  |u|248dub Finding cid:98EY |X = u requires calculating some moments. By LOTUS:= 2du =480.1961  ucid:90 cid:90 cid:90 1cid:90 2v1cid:90 1v102vdudv2v2dv =023,vfX,Y u, vdudvEY  ===cid:90 2v1cid:90 1cid:90 100EXY  ==2uvdudvv1v2v  12  v  12dv =cid:90 103v3  2v2dv =112,EX 2 = 2u21  udu =16.cid:90 10A glance at fX shows EX = 0, so VarX = EX 2 = 1by 4.35,cid:98EY |X = u = Lu =This estimator is shown in Figure 4.24b. While the exact calculation of cid:98EY |X = u was tedious,u =++.its graph could have been drawn approximately by inspection. It is a straight line that tries to beclose to EY |X = u for all u. To nd the MSE we shall use 4.38. By LOTUS,231/121/623u26 and CovX, Y  = EXY  = 112 . Therefore,cid:104cid:98EY |X = u2cid:105= Eand Ecid:104cid:0 23 + X2cid:90 2v1v1cid:90 1cid:90 10EY 2 =2v2dudv.0=12= 42v3dv =9 + EX 2cid:12cid:105MSE for cid:98EY |X =96  MSE using cid:98EY |X = 172 . Thus, 35724 = 3517212=.Note that MSE using EY |X = 1MSEs are ordered in accordance with 4.39.72  VarY  = 118 , so the three4.10. LAW OF LARGE NUMBERS AND CENTRAL LIMIT THEOREM2114.10 Law of large numbers and central limit theoremThe law of large numbers, in practical applications, has to do with approximating sums of randomvariables by a constant. The Gaussian approximation, backed by the central limit theorem, inpractical applications, has to do with a more rened approximation: approximating sums of randomvariables by a single Gaussian random variable.4.10.1 Law of large numbersThere are many forms of the law of large numbers LLN. The law of large numbers is about thesample average of n random variables: Snn , where Sn = X1 + . . . + Xn. The random variables havethe same mean, . The random variables are assumed to be independent, or weakly dependent, andsome condition is placed on the sizes of the individual random variables. The conclusion is that asn  , Snn converges in some sense to the mean, . The following version of the LLN has a simpleproof.Proposition 4.10.1 Law of large numbers Suppose X1, X2, . . . is a sequence of uncorrelatedrandom variables such that each Xk has nite mean  and variance less than or equal to C. Thenfor any  > 0,Proof. The mean of Snn is given byP  Cn2n 0.cid:26cid:12cid:12cid:12cid:12 Sncid:20cid:80nncid:18cid:80ncid:27cid:12cid:12cid:12cid:12  cid:21cid:80n=cid:19=cid:21Ecid:20 Sncid:19cid:18 Snnn= Ek=1 Xknk=1 EXkn=nn= .The variance of Snn is bounded above by:Var= Vark=1 Xkncid:80nk=1 VarXkn2 nCn2 =Cn.Therefore, the proposition follows from the Chebychev inequality, 2.12, applied to the randomvariable Snn .The law of large numbers is illustrated in Figure 4.25, which was made using a random numbergenerator on a computer. For each n  1, Sn is the sum of the rst n terms of a sequence ofindependent random variables, each uniformly distributed on the interval 0, 1. Figure 4.25aillustrates the statement of the LLN, indicating convergence of the averages, Snn , towards the mean,0.5, of the individual uniform random variables. The same sequence Sn is shown in Figure 4.25b,except the Sns are not divided by n. The sequence of partial sums Sn converges to +. The LLNtells us that the asymptotic slope is equal to 0.5. The sequence Sn is not expected to get closer ton2 as n increasesjust to have the same asymptotic slope. In fact, the central limit theorem, givenin the next section, implies that for large n, the dierence Sn  n2 has approximately the Gaussiandistribution with mean zero, variance n12 , and standard deviationcid:112 n12 .212CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESFigure 4.2