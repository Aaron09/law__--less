iven an observation, such as X = 2,the maximum a posteriori MAP decision rule chooses the hypothesis with the larger conditionalprobability. By Bayes formula, P H1|X = 2 = P H1,X=2That is, P H1|X = 2 is the top number in the column for X = 2 in the joint probability matrixdivided by the sum of the numbers in the column for X = 2. Similarly the conditional probabilityP H0|X = 2 is the bottom number in the column for X = 2 divided by the sum of the numbersin the column for X = 2. Since the denominators are the same both denominators are equal toPX = 2 it follows that whether P H1|X = 2 > P H0|X = 2 is equivalent to whether the topentry in the column for X = 2 is greater than the bottom entry in the column for X = 2.P H1,X=2+P H0,X=2 = 0.06P X=2 =P H1,X=20.06+0.16Thus, the MAP decision rule can be specied by underlining the larger entry in each column ofthe joint probability matrix. For our original example, the MAP rule is given byX = 0 X = 1 X = 2 X = 30.000.120.080.320.060.160.020.24H1H0 underlines indicatethe MAP decision rule .Thus, if the observation is X = k, the MAP rule declares hypothesis H1 is true if 1p1k > 0p0k,or equivalently if k > 0, where  is the likelihood ratio dened above. Therefore, the MAP1rule is equivalent to the LRT with threshold  = 01If 1 = 0, the prior is said to be uniform, because it means the hypotheses are equally likely.For the uniform prior the threshold for the MAP rule is one, and the MAP rule is the same as theML rule. Does it make sense that if 0 > 1, then the threshold for the MAP rule in LRT formis greater than one? Indeed it does, because a larger threshold value in the LRT means there arefewer observations leading to deciding H1 is true, which is appropriate behavior if 0 > 1..The MAP rule has a remarkable optimality property, as we now explain. The average errorprobability, which we call pe, for any decision rule can be written as pe = 0pfalse alarm + 1pmiss.A decision rule is specied by underlining one number from each column of the joint probabilitymatrix. The corresponding pe is the sum of all numbers in the joint probability matrix that arenot underlined. From this observation it easily follows that, among all decision rules, the MAP64CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESdecision rule is the one that minimizes pe. That is why some books call the MAP rule the minimumprobability of error rule.Examples are given in the remainder of this section, illustrating the use of ML and MAP decisionrules for hypothesis testing.Example 2.11.1 Suppose you have a coin and you know that either : H1: the coin is biased,showing heads on each ip with probability 2/3; or H0: the coin is fair. Suppose you ip the coinve times. Let X be the number of times heads shows. Describe the ML and MAP decision rules,and nd pfalse alarm, pmiss, and pe for both of them, using the prior 0, 1 = 0.2, 0.8 for the MAPrule and for dening pe for both rules.Solution: The rows of the likelihood matrix consist of the pmf of the binomial distribution withn = 5 and p = 2/3 for H1 and p = 1/2 for H0 :In computing the likelihood ratio, the binomial coecients cancel, soTherefore, the ML decision rule is to declare H1 whenever X  1, or equivalently, X  3. Forthe ML rule,X = 0cid:15cid:15cid:0 1cid:0 132H1H0233332X = 2X = 1cid:0 2cid:1cid:0 1cid:12cid:0 1cid:14 10cid:0 25cid:0 210cid:0 15cid:0 1cid:15cid:15cid:15kcid:1kcid:0 1cid:15cid:0 1cid:195cid:18 1cid:19cid:18 1cid:18 2cid:18 1cid:195cid:18 1cid:195k =+ 522332X = 5cid:15cid:0 2cid:15 .cid:0 132cid:1X = 4cid:14cid:0 1cid:12 5cid:0 25cid:0 1cid:1533233X = 3cid:13cid:0 1cid:13 10cid:0 210cid:0 1cid:15cid:195  2kcid:18 2= 2k23.7.6cid:18 12cid:195cid:18 2+cid:194= 0.5cid:192cid:18 1cid:193=51243 0.2013pfalse alarm = 10pmiss =pe = 0.2pfalse alarm + 0.8pmiss  0.26.+ 10+ 53333The MAP decision rule is to declare H1 whenever X  0.25, or equivalently, X  1. That is,the MAP rule declares H0 only if X = 0. For the MAP rule,pfalse alarm = 1 cid:195  0.97cid:18 1cid:1952cid:18 1pmiss =pe = 0.2pfalse alarm + 0.8pmiss  0.227.=3 0.0411243As expected, pe for the MAP rule designed with the correct prior probabilities in mind is smallerthan pe for the ML rule.2.11. BINARY HYPOTHESIS TESTING WITH DISCRETE-TYPE OBSERVATIONS65Example 2.11.2 Detection problem with Poisson distributed observations A certain deep spacetransmitter uses on-o modulation of a laser to send a bit, with value either zero or one. If the bitis zero, the number of photons, X, arriving at the receiver has the Poisson distribution with mean0 = 2; and if the bits is one, X has the Poisson distribution with mean 1 = 6. A decision ruleis needed to decide, based on observation of X, whether the bit was a zero or a one. Describe athe ML decision rule, and b the MAP decision rule under the assumption that sending a zero isa priori ve times more likely than sending a one i.e. 0/1 = 5. Express both rules as directlyin terms of X as possible.Solution: The ML rule is to decide a one is sent if X  1, where  is the like