nuous Jacobian with a nonzero determinant, but there are multiple points in the u  vplane mapping to a single point in the    plane, then Proposition 4.7.4 requires a modicationin order to apply. Namely, the pdf of fW,Z,  is given by a sum of terms, with the sum runningover points of the form ui, vi such that gui, vi = , . This modication is illustrated by anexample for which the mapping is two-to-one.Example 4.7.8 Suppose W = minX, Y  and Z = maxX, Y , where X and Y are jointlycontinuous-type random variables. Express fW,Z in terms of fX,Y .cid:82 Solution: Note that W, Z is the image of X, Y  under the mapping from the u  v plane tothe   plane dened by  = minu, v and  = maxu, v, shown in Figure 4.21. This mappingmaps R2 into the set ,  :   , which is the set of points on or above the diagonal i.e. theline  =  in the    plane. Since X and Y are jointly continuous, PW = Z = PX = Y  =v fX,Y u, vdudv = 0, so it does not matter how the joint density of W, Z is dened exactlyon the diagonal; we will set it to zero there. Let U = ,  :  < , which is the region that liesstrictly above the diagonal. For any subset A  U, W, Z  A = X, Y   A  Y, X  A,cid:82 v!4.8. CORRELATION AND COVARIANCE195Figure 4.21: The mapping of u, v to minu, v, maxu, v.where the two sets in this union are disjoint. Therefore, using the fact fY,X u, v = fX,Y v, u,PW, Z  A = PX, Y   A + PY, X  AfX,Y u, v + fY,X u, vdudvfX,Y u, v + fX,Y v, ududvfX,Y ,  + fX,Y , dd,==cid:90 cid:90cid:90 cid:90cid:90 cid:90cid:26 fX,Y ,  + fX,Y ,   < =AAA  .where in the last step we simply changed the variables of integration. Consequently, the joint pdfof fW,Z is given byfW,Z,  =0There are two terms on the right hand side because for each ,  with  <  there are two pointsin the u, v plane that map into that point: ,  and , . No Jacobian factors such as thosein Section 4.7.2 appear for this example because the mappings u, v  u, v and u, v  v, uboth have Jacobians with determinant equal to one. Geometrically, to get fW,Z from fX,Y imaginespreading the probability mass for X, Y  on the plane, and then folding the plane at the diagonalby swinging the part of the plane below the diagonal to above the diagonal, and then addingtogether the two masses above the diagonal. This interpretation is similar to the one found for thepdf of |X|, in Example 3.8.8.4.8 Correlation and covarianceThe rst and second moments, or equivalently, the mean and variance, of a single random variableconvey important information about the distribution of the variable, and the moments are oftensimpler to deal with than pmfs, pdfs, or CDFs. Use of moments is even more important whenconsidering more than one random variable at a time. That is because joint distributions are muchmore complex than distributions for individual random variables.u!vU196CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESLet X and Y be random variables with nite second moments. Three important related quan-tities are:the correlation: EXY the covariance: CovX, Y  = EX  EXY  EY the correlation coecient: X,Y =cid:112VarXVarY CovX, Y =CovX, Y X Y.Covariance generalizes variance, in the sense that VarX = CovX, X. Recall that there areuseful shortcuts for computing variance: VarX = EXX  EX = EX 2  EX2. Similarshortcuts exist for computing covariances:CovX, Y  = EXY  EY  = EX  EXY  = EXY   EXEY .In particular, if either X or Y has mean zero, then EXY  = CovX, Y .Random variables X and Y are called uncorrelated if CovX, Y  = 0. If VarX > 0 andVarY  > 0, so that X,Y is well dened, then X and Y being uncorrelated is equivalent toX,Y = 0. If CovX, Y  > 0, or equivalently, X,Y > 0, the variables are said to be positivelycorrelated, and if CovX, Y  < 0, or equivalently, X,Y < 0, the variables are said to be negativelycorrelated.If X and Y are independent, then EXY  = EXEY , which implies that X andY are uncorrelated. The converse is falseuncorrelated does not imply independenceand in fact,independence is a much stronger condition than being uncorrelated. Specically, independencerequires a large number of equations to hold, namely FXY u, v = FX uFY v for every real valueof u and v. The condition of being uncorrelated requires only a single equation to hold.Three or more random variables are said to be uncorrelated if they are pairwise uncorrelated.That is, there is no dierence between a set of random variables being uncorrelated or being pairwiseuncorrelated. Recall from Section 2.4.1 that, in contrast, independence of three or more events isa stronger property than pairwise independence. Therefore, mutual independence of n randomvariables is a stronger property than pairwise independence. Pairwise independence of n randomvariables implies that they are uncorrelated.Covariance is linear in each of its two arguments, and adding a constant to a random variabledoes not change the covariance of that random variable with other random variables:CovX + Y, U + V  = CovX, U  + CovX, V  + CovY, U  + CovY, V CovaX + b, cY + d = acCovX, Y ,for constannuous Jacobian with a nonzero determinant, but there are multiple points in the u  vplane mapping to a single point in the    plane, then Proposition 4.7.4 requires a modicationin order to apply. Namely, the pdf of fW,Z,  is given by a sum of terms, with the sum runningover points of the form ui, vi such that gui, vi = , . This modication is illustrated by anexample for which the mapping is two-to-one.Example 4.7.8 Suppose W = minX, Y  and Z = maxX, Y , where X and Y are jointlycontinuous-type random variables. Express fW,Z in terms of fX,Y .cid:82 Solution: Note that W, Z is the image of X, Y  under the mapping from the u  v plane tothe   plane dened by  = minu, v and  = maxu, v, shown in Figure 4.21. This mappingmaps R2 into the set ,  :   , which is the set of points on or above the diagonal i.e. theline  =  in the    plane. Since X and Y are jointly continuous, PW = Z = PX = Y  =v fX,Y u, vdudv = 0, so it does not matter how the joint density of W, Z is dened exactlyon the diagonal; we will set it to zero there. Let U = ,  :  < , which is the region that liesstrictly above the diagonal. For any subset A  U, W, Z  A = X, Y   A  Y, X  A,cid:82 v!4.8. CORRELATION AND COVARIANCE195Figure 4.21: The mapping of u, v to minu, v, maxu, v.where the two sets in this union are disjoint. Therefore, using the fact fY,X u, v = fX,Y v, u,PW, Z  A = PX, Y   A + PY, X  AfX,Y u, v + fY,X u, vdudvfX,Y u, v + fX,Y v, ududvfX,Y ,  + fX,Y , dd,==cid:90 cid:90cid:90 cid:90cid:90 cid:90cid:26 fX,Y ,  + fX,Y ,   < =AAA  .where in the last step we simply changed the variables of integration. Consequently, the joint pdfof fW,Z is given byfW,Z,  =0There are two terms on the right hand side because for each ,  with  <  there are two pointsin the u, v plane that map into that point: ,  and , . No Jacobian factors such as thosein Section 4.7.2 appear for this example because the mappings u, v  u, v and u, v  v, uboth have Jacobians with determinant equal to one. Geometrically, to get fW,Z from fX,Y imaginespreading the probability mass for X, Y  on the plane, and then folding the plane at the diagonalby swinging the part of the plane below the diagonal to above the diagonal, and then addingtogether the two masses above the diagonal. This interpretation is similar to the one found for thepdf of |X|, in Example 3.8.8.4.8 Correlation and covarianceThe rst and second moments, or equivalently, the mean and variance, of a single random variableconvey important information about the distribution of the variable, and the moments are oftensimpler to deal with than pmfs, pdfs, or CDFs. Use of moments is even more important whenconsidering more than one random variable at a time. That is because joint distributions are muchmore complex than distributions for individual random variables.u!vU196CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESLet X and Y be random variables with nite second moments. Three important related quan-tities are:the correlation: EXY the covariance: CovX, Y  = EX  EXY  EY the correlation coecient: X,Y =cid:112VarXVarY CovX, Y =CovX, Y X Y.Covariance generalizes variance, in the sense that VarX = CovX, X. Recall that there areuseful shortcuts for computing variance: VarX = EXX  EX = EX 2  EX2. Similarshortcuts exist for computing covariances:CovX, Y  = EXY  EY  = EX  EXY  = EXY   EXEY .In particular, if either X or Y has mean zero, then EXY  = CovX, Y .Random variables X and Y are called uncorrelated if CovX, Y  = 0. If VarX > 0 andVarY  > 0, so that X,Y is well dened, then X and Y being uncorrelated is equivalent toX,Y = 0. If CovX, Y  > 0, or equivalently, X,Y > 0, the variables are said to be positivelycorrelated, and if CovX, Y  < 0, or equivalently, X,Y < 0, the variables are said to be negativelycorrelated.If X and Y are independent, then EXY  = EXEY , which implies that X andY are uncorrelated. The converse is falseuncorrelated does not imply independenceand in fact,independence is a much stronger condition than being uncorrelated. Specically, independencerequires a large number of equations to hold, namely FXY u, v = FX uFY v for every real valueof u and v. The condition of being uncorrelated requires only a single equation to hold.Three or more random variables are said to be uncorrelated if they are pairwise uncorrelated.That is, there is no dierence between a set of random variables being uncorrelated or being pairwiseuncorrelated. Recall from Section 2.4.1 that, in contrast, independence of three or more events isa stronger property than pairwise independence. Therefore, mutual independence of n randomvariables is a stronger property than pairwise independence. Pairwise independence of n randomvariables implies that they are uncorrelated.Covariance is linear in each of its two arguments, and adding a constant to a random variabledoes not change the covariance of that random variable with other random variables:CovX + Y, U + V  = CovX, U  + CovX, V  + CovY, U  + CovY, V CovaX + b, cY + d = acCovX, Y ,for constan