lihood ratiofunction, dened byk =P X = k|one is sentP X = k|zero is sent=e1ke0k1/k!0/k!=e10 = 3ke4  3k54.6.cid:18 1cid:19k0Therefore, the ML decision rule is to decide a one is sent if X  4.The MAP rule is to decide a one is sent if X  0found. So the MAP rule with 0 = 51 decides a one is sent if 3XNote that when X = 4 or X = 5, the ML rule decides that a one was transmitted, not a zero, butbecause zeroes are so much more likely to be transmitted than ones, the MAP rule decides in favorof a zero in this case.54.6  5, or equivalently, if X  6., where  is the likelihood ratio already1Example 2.11.3 Sensor fusion Two motion detectors are used to detect the presence of a personin a room, as part of an energy saving temperature control system. The rst sensor outputs a valueX and the second sensor outputs a value Y . Both outputs have possible values 0, 1, 2, with largernumbers tending to indicate that a person is present. Let H0 be the hypothesis a person is absentand H1 be the hypothesis a person is present. The likelihood matrices for X and for Y are shown:X = 0 X = 1 X = 2Y = 0 Y = 1 Y = 2H1H00.10.80.30.10.60.1H1H00.10.70.10.20.80.1For example, P Y = 2|H1 = 0.8. Suppose, given one of the hypotheses is true, the sensors provideconditionally independent readings, so thatP X = i, Y = j|Hk = P X = i|HkP Y = j|Hk for i, j  0, 1, 2 and k  0, 1.a Find the likelihood matrix for the observation X, Y  and indicate the ML decision rule. To bedenite, break ties in favor of H1.b Find pfalse alarm and pmiss for the ML rule found in part a.c Suppose, based on past experience, prior probabilities 1 = P H1 = 0.2 and 0 = P H0 = 0.8are assigned. Compute the joint probability matrix and indicate the MAP decision rule.66CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESd For the MAP decision rule, compute pfalse alarm, pmiss, and the unconditional probability of errorpe = 0pfalse alarm + 1pmiss.e Using the same priors as in part c, compute the unconditional error probability, pe, for theML rule from part a. Is it smaller or larger than pe found for the MAP rule in d?Solution:a The likelihood matrix for observation X, Y  is the following.X, Y   0, 00.010.561, 00.030.070, 10.010.160, 20.080.081, 10.030.022, 00.060.071, 20.240.012, 10.060.02H1H02, 20.480.01.The ML decisions are indicated by the underlined elements. The larger number in each column isunderlined, with the tie in case 0, 2 broken in favor of H1, as specied in the problem statement.Note that the row sums are both one.b For the ML rule, pfalse alarm is the sum of the entries in the row for H0 in the likelihood matrixthat are not underlined. So pfalse alarm = 0.08 + 0.02 + 0.01 + 0.02 + 0.01 = 0.14.For the ML rule, pmiss is the sum of the entries in the row for H1 in the likelihood matrix that arenot underlined. So pmiss = 0.01 + 0.01 + 0.03 + 0.06 = 0.11.c The joint probability matrix is given byX, Y   0, 00, 20, 12, 22, 10.002 0.002 0.016 0.006 0.006 0.048 0.012 0.0120.0960.448 0.128 0.064 0.056 0.016 0.008 0.056 0.016 0.008.1, 11, 02, 01, 2H1H0The matrix species P X = i, Y = j, Hk for each hypothesis Hk and for each possible observationvalue i, j. The 18 numbers in the matrix sum to one. The MAP decisions are indicated bythe underlined elements in the joint probability matrix. The larger number in each column isunderlined.d For the MAP rule,pfalse alarm = P X, Y   1, 2, 2, 2|H0 = 0.01 + 0.01 = 0.02,andpmiss = P X, Y  cid:54 1, 2, 2, 2|H1 = 1 P X, Y   1, 2, 2, 2|H1 = 1 0.24 0.48 = 0.28.Thus, for the MAP rule, pe = 0.80.02 + 0.20.28 = 0.072. This pe is also the sum of theprobabilities in the joint probability matrix that are not underlined.e Using the conditional probabilities found in a and the given values of 0 and 1 yields thatfor the ML rule: pe = 0.80.14 + 0.20.11 = 0.134, which is larger than the value 0.072 for theMAP rule, as expected because of the optimality of the MAP rule for the given priors.2.12. RELIABILITY2.12 Reliability67Reliability of complex systems is of central importance to many engineering design problems. Exten-sive terminology, models, and graphical representations have been developed within many dierentelds of engineering, from construction of major structures to logistics of complex operations. Acommon theme is to try to evaluate the reliability of a large system by recursively evaluating thereliability of its subsystems. Often no more underlying probability theory is required beyond thatcovered earlier in this chapter. However, intuition can be sharpened by considering the case thatmany of the events have very small probabilities.2.12.1 Union boundA general tool for bounding failure probabilities is the following. Given two events A and B, theunion bound isP A  B  P A + P B.A proof of the bound is that P A + P B  P A  B = P AB  0. If the bound, P A + P B,is used as an approximation to P A  B, the error or gap is P AB. If A and B have largeprobabilities, the gap can be signicant; P A + P B might even be larger than one. However, ingeneral, P AB  minP lihood ratiofunction, dened byk =P X = k|one is sentP X = k|zero is sent=e1ke0k1/k!0/k!=e10 = 3ke4  3k54.6.cid:18 1cid:19k0Therefore, the ML decision rule is to decide a one is sent if X  4.The MAP rule is to decide a one is sent if X  0found. So the MAP rule with 0 = 51 decides a one is sent if 3XNote that when X = 4 or X = 5, the ML rule decides that a one was transmitted, not a zero, butbecause zeroes are so much more likely to be transmitted than ones, the MAP rule decides in favorof a zero in this case.54.6  5, or equivalently, if X  6., where  is the likelihood ratio already1Example 2.11.3 Sensor fusion Two motion detectors are used to detect the presence of a personin a room, as part of an energy saving temperature control system. The rst sensor outputs a valueX and the second sensor outputs a value Y . Both outputs have possible values 0, 1, 2, with largernumbers tending to indicate that a person is present. Let H0 be the hypothesis a person is absentand H1 be the hypothesis a person is present. The likelihood matrices for X and for Y are shown:X = 0 X = 1 X = 2Y = 0 Y = 1 Y = 2H1H00.10.80.30.10.60.1H1H00.10.70.10.20.80.1For example, P Y = 2|H1 = 0.8. Suppose, given one of the hypotheses is true, the sensors provideconditionally independent readings, so thatP X = i, Y = j|Hk = P X = i|HkP Y = j|Hk for i, j  0, 1, 2 and k  0, 1.a Find the likelihood matrix for the observation X, Y  and indicate the ML decision rule. To bedenite, break ties in favor of H1.b Find pfalse alarm and pmiss for the ML rule found in part a.c Suppose, based on past experience, prior probabilities 1 = P H1 = 0.2 and 0 = P H0 = 0.8are assigned. Compute the joint probability matrix and indicate the MAP decision rule.66CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESd For the MAP decision rule, compute pfalse alarm, pmiss, and the unconditional probability of errorpe = 0pfalse alarm + 1pmiss.e Using the same priors as in part c, compute the unconditional error probability, pe, for theML rule from part a. Is it smaller or larger than pe found for the MAP rule in d?Solution:a The likelihood matrix for observation X, Y  is the following.X, Y   0, 00.010.561, 00.030.070, 10.010.160, 20.080.081, 10.030.021, 20.240.012, 00.060.072, 10.060.02H1H02, 20.480.01.The ML decisions are indicated by the underlined elements. The larger number in each column isunderlined, with the tie in case 0, 2 broken in favor of H1, as specied in the problem statement.Note that the row sums are both one.b For the ML rule, pfalse alarm is the sum of the entries in the row for H0 in the likelihood matrixthat are not underlined. So pfalse alarm = 0.08 + 0.02 + 0.01 + 0.02 + 0.01 = 0.14.For the ML rule, pmiss is the sum of the entries in the row for H1 in the likelihood matrix that arenot underlined. So pmiss = 0.01 + 0.01 + 0.03 + 0.06 = 0.11.c The joint probability matrix is given byX, Y   0, 00, 20, 12, 22, 10.002 0.002 0.016 0.006 0.006 0.048 0.012 0.0120.0960.448 0.128 0.064 0.056 0.016 0.008 0.056 0.016 0.008.1, 11, 01, 22, 0H1H0The matrix species P X = i, Y = j, Hk for each hypothesis Hk and for each possible observationvalue i, j. The 18 numbers in the matrix sum to one. The MAP decisions are indicated bythe underlined elements in the joint probability matrix. The larger number in each column isunderlined.d For the MAP rule,pfalse alarm = P X, Y   1, 2, 2, 2|H0 = 0.01 + 0.01 = 0.02,andpmiss = P X, Y  cid:54 1, 2, 2, 2|H1 = 1 P X, Y   1, 2, 2, 2|H1 = 1 0.24 0.48 = 0.28.Thus, for the MAP rule, pe = 0.80.02 + 0.20.28 = 0.072. This pe is also the sum of theprobabilities in the joint probability matrix that are not underlined.e Using the conditional probabilities found in a and the given values of 0 and 1 yields thatfor the ML rule: pe = 0.80.14 + 0.20.11 = 0.134, which is larger than the value 0.072 for theMAP rule, as expected because of the optimality of the MAP rule for the given priors.2.12. RELIABILITY2.12 Reliability67Reliability of complex systems is of central importance to many engineering design problems. Exten-sive terminology, models, and graphical representations have been developed within many dierentelds of engineering, from construction of major structures to logistics of complex operations. Acommon theme is to try to evaluate the reliability of a large system by recursively evaluating thereliability of its subsystems. Often no more underlying probability theory is required beyond thatcovered earlier in this chapter. However, intuition can be sharpened by considering the case thatmany of the events have very small probabilities.2.12.1 Union boundA general tool for bounding failure probabilities is the following. Given two events A and B, theunion bound isP A  B  P A + P B.A proof of the bound is that P A + P B  P A  B = P AB  0. If the bound, P A + P B,is used as an approximation to P A  B, the error or gap is P AB. If A and B have largeprobabilities, the gap can be signicant; P A + P B might even be larger than one. However, ingeneral, P AB  minP 