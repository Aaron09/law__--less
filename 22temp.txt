istributions have two parameters, namely n and p, and they involve binomial coef-cients, which can be cumbersome. Poisson distributions are simplerhaving only one parameter,, and no binomial coecients. So it is worthwhile using the Poisson distribution rather than thebinomial distribution for large n and small p. We now derive a limit result to give evidence thatthis is a good approximation. Let  > 0, let n and k be integers with n   and 0  k  n, and letpbk denote the probability mass at k of the binomial distribution with parameters n and p = /n.We rst consider the limit of the mass of the binomial distribution at k = 0. Note thatln pb0 = ln1  pn = n ln1  p = n lncid:181  ncid:19.  as n  .By Taylors theorem, ln1 + u = u + ou where ou/u  0 as u  0. So, using u =  n ,Therefore,ln pb0 = npb0 =cid:18 ncid:18+ o1  n ncid:19cid:19cid:18cid:19n  eas n  ,2.9so the probability mass at zero for the binomial distribution converges to the probability mass atzero for the Poisson distribution. Similarly, for any integer k  0 xed,cid:18ncid:19pbk ===pk1  pnkkn  n  1 n  k + 1cid:18 cid:19kcid:18cid:20 n  n  1 n  k + 1k!nkpb0k!nkcid:19nkcid:21cid:34cid:181  n1  ncid:19kcid:352.10 kek!as n  ,because 2.9 holds, and the terms in square brackets in 2.10 converge to one.Checking that the Poisson distribution sums to one, and deriving the mean and variance of thePoisson distribution, can be done using the pmf as can be done for the binomial distribution. This2.8. MAXIMUM LIKELIHOOD PARAMETER ESTIMATION47is not surprising, given that the Poisson distribution is a limiting form of the binomial distribution.The Maclaurin series for ex plays a role:cid:88k=0ex =xkk!.2.11Letting x = , and dividing both sides of 2.11 by e, yieldscid:88k=01 =kek!,so the pmf does sum to one. Following 2.7 line for line yields that if Y has the Poisson distributionwith parameter ,EY  ==kkk=0cid:88cid:88cid:88cid:88k=1k=1kek!kek!k1ek  1!lel!l=0= = here l = k  1= .Similarly, it can be shown that VarY  = . The mean and variance can be obtained by takingthe limit of the mean and limit of the variance of the binomial distribution with parameters n andp = /n, as n  , as follows. The mean of the Poisson distribution is limn n n = , and thevariance of the Poisson distribution is limn n ncid:01  cid:1 = .n2.8 Maximum likelihood parameter estimationSometimes when we devise a probability model for some situation we have a reason to use aparticular type of probability distribution, but there may be a parameter that has to be selected.A common approach is to collect some data and then estimate the parameter using the observeddata. For example, suppose we decide that an experiment is accurately modeled by a probabilitymodel with a random variable X, and that the pmf of X is p, where  is a parameter, but thevalue of the parameter is not known before the experiment is performed. When the experiment isperformed, suppose we observe a particular value k for X. According to the probability model, theprobability of k being the observed value for X, before the experiment was performed, would havebeen pk. It is said that the likelihood that X = k is pk. The maximum likelihood estimate of 48for observation k, denoted by cid:98M Lk, is the value of  that maximizes the likelihood, pk, withCHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESrespect to . Intuitively, the maximum likelihood estimate is the value of  that best explains theobserved value k, or makes it the least surprising.A way to think about it is that pk depends on two variables: the parameter  and the valueof the observed value k for X. It is the likelihood that X = k, which depends on . For parameterestimation, the goal is to come up with an estimate of the parameter  for a given value k of theobserved value. It wouldnt make sense to maximize the likelihood with respect to the observedvalue k, because k is assumed to be given. Rather, the maximization is performed with respect tothe unknown parameter value; the maximum likelihood estimate is the value of the parameter thatmaximizes the likelihood of the observed value.In the following context it makes sense to maximize pk with respect to k for  xed. Supposeyou know  and then you enter into a guessing game, in which you guess what the value of X willbe, before the experiment is performed. If your guess is k, then your probability of winning is pk,so you would maximize your probability of winning by guessing the value of k that maximizes pk.For parameter estimation, the value k is observed and the likelihood is maximized with respect to; for the guessing game, the parameter  is known and the likelihood is maximized with respectto k.Example 2.8.1 Suppose a bent coin is given to a student. The coin is badly bent, but the studentcan still ip the coin and see whether it shows heads or tails. The coin shows heads with probabilityp each time it is ipped. The student ips the coin n times for some large value of n for example,n = 1000 is reasonable. Heads shows on k of the ips. Finistributions have two parameters, namely n and p, and they involve binomial coef-cients, which can be cumbersome. Poisson distributions are simplerhaving only one parameter,, and no binomial coecients. So it is worthwhile using the Poisson distribution rather than thebinomial distribution for large n and small p. We now derive a limit result to give evidence thatthis is a good approximation. Let  > 0, let n and k be integers with n   and 0  k  n, and letpbk denote the probability mass at k of the binomial distribution with parameters n and p = /n.We rst consider the limit of the mass of the binomial distribution at k = 0. Note thatln pb0 = ln1  pn = n ln1  p = n lncid:181  ncid:19.  as n  .By Taylors theorem, ln1 + u = u + ou where ou/u  0 as u  0. So, using u =  n ,Therefore,ln pb0 = npb0 =cid:18 ncid:18+ o1  n ncid:19cid:19cid:18cid:19n  eas n  ,2.9so the probability mass at zero for the binomial distribution converges to the probability mass atzero for the Poisson distribution. Similarly, for any integer k  0 xed,cid:18ncid:19pbk ===pk1  pnkkn  n  1 n  k + 1cid:18 cid:19kcid:18cid:20 n  n  1 n  k + 1k!nkpb0k!nkcid:19nkcid:21cid:34cid:181  n1  ncid:19kcid:352.10 kek!as n  ,because 2.9 holds, and the terms in square brackets in 2.10 converge to one.Checking that the Poisson distribution sums to one, and deriving the mean and variance of thePoisson distribution, can be done using the pmf as can be done for the binomial distribution. This2.8. MAXIMUM LIKELIHOOD PARAMETER ESTIMATION47is not surprising, given that the Poisson distribution is a limiting form of the binomial distribution.The Maclaurin series for ex plays a role:cid:88k=0ex =xkk!.2.11Letting x = , and dividing both sides of 2.11 by e, yieldscid:88k=01 =kek!,so the pmf does sum to one. Following 2.7 line for line yields that if Y has the Poisson distributionwith parameter ,EY  ==kkk=0cid:88cid:88cid:88cid:88k=1k=1kek!kek!k1ek  1!lel!l=0= = here l = k  1= .Similarly, it can be shown that VarY  = . The mean and variance can be obtained by takingthe limit of the mean and limit of the variance of the binomial distribution with parameters n andp = /n, as n  , as follows. The mean of the Poisson distribution is limn n n = , and thevariance of the Poisson distribution is limn n ncid:01  cid:1 = .n2.8 Maximum likelihood parameter estimationSometimes when we devise a probability model for some situation we have a reason to use aparticular type of probability distribution, but there may be a parameter that has to be selected.A common approach is to collect some data and then estimate the parameter using the observeddata. For example, suppose we decide that an experiment is accurately modeled by a probabilitymodel with a random variable X, and that the pmf of X is p, where  is a parameter, but thevalue of the parameter is not known before the experiment is performed. When the experiment isperformed, suppose we observe a particular value k for X. According to the probability model, theprobability of k being the observed value for X, before the experiment was performed, would havebeen pk. It is said that the likelihood that X = k is pk. The maximum likelihood estimate of 48for observation k, denoted by cid:98M Lk, is the value of  that maximizes the likelihood, pk, withCHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESrespect to . Intuitively, the maximum likelihood estimate is the value of  that best explains theobserved value k, or makes it the least surprising.A way to think about it is that pk depends on two variables: the parameter  and the valueof the observed value k for X. It is the likelihood that X = k, which depends on . For parameterestimation, the goal is to come up with an estimate of the parameter  for a given value k of theobserved value. It wouldnt make sense to maximize the likelihood with respect to the observedvalue k, because k is assumed to be given. Rather, the maximization is performed with respect tothe unknown parameter value; the maximum likelihood estimate is the value of the parameter thatmaximizes the likelihood of the observed value.In the following context it makes sense to maximize pk with respect to k for  xed. Supposeyou know  and then you enter into a guessing game, in which you guess what the value of X willbe, before the experiment is performed. If your guess is k, then your probability of winning is pk,so you would maximize your probability of winning by guessing the value of k that maximizes pk.For parameter estimation, the value k is observed and the likelihood is maximized with respect to; for the guessing game, the parameter  is known and the likelihood is maximized with respectto k.Example 2.8.1 Suppose a bent coin is given to a student. The coin is badly bent, but the studentcan still ip the coin and see whether it shows heads or tails. The coin shows heads with probabilityp each time it is ipped. The student ips the coin n times for some large value of n for example,n = 1000 is reasonable. Heads shows on k of the ips. Fin