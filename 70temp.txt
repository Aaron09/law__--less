denote the length ofthe middle subinterval. If the unit interval represents a stick, and the two points represent locationsof breaks, then X and Y are the lengths of the left and center sticks formed. See Figure 4.11. FindFigure 4.11: Unit interval divided into intervals of length X, Y , and 1  X  Y.the pdf of Y.Solution: We shall rst determine the joint pdf of X and Y , and then integrate it to nd themarginal pdf, fY . Basically we will be using the version of the law of total probability for pdfsin 4.10. The pdf of X is simpleX is uniformly distributed over the interval 0, 1. The supportof fX,Y is the triangular region, T = u, v : u  0, v  0, and u + v  1. In particular,fX,Y u, v = 0 if u  0 or if u  1. For 0 < u < 1, given X = u, the set of possible values of Y isYvf     u|v  ouXf  uf  vuuvvoX|YY|Xf     v|u  ooY01X174CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESthe interval 0, 1  u. Since the second random point is uniformly distributed over u, 1, and Y isthe dierence between that point and the constant u, the conditional distribution of Y is uniformover the interval 0, 1  u. That is, if 0 < u < 1:fY |X v|u =0  v  1  uelse.For 0 < u < 1, fX,Y u, v = fX ufY |X v|u = fX|Y v|u, and fX,Y u, v = 0 otherwise. Therefore,cid:26 11u0cid:26 1cid:90 1vo0cid:12cid:12cid:12cid:121vo0u, v  Telse.The support of the pdf of Y is 0, 1 so let vo  0, 1. ThenfX,Y u, v =1u0cid:90 fY vo =fX,Y u, vodu =11  udu =  ln1  u=  lnvo.This is innite if vo = 0, but the density at a single point doesnt make a dierence, so we setfY 0 = 0. Thus, we have:cid:26  lnv 0 < v < 10else.fY v =Example 4.3.6 Let Z = YX 2 , such that X and Y have joint pdf given bycid:4010fX,Y u, v =if 0  u  1, 0  v  1else.a Find the numerical value of PZ  0.5. Also, sketch a region in the plane so that the area ofthe region is PZ  0.5. b Find the numerical value of PZ  4. Also, sketch a region in theplane so that the area of the region is PZ  4.a PZ  0.5 = PY  0.5X 2 =cid:82 1Solution.0 0.5u2du = 16 .b PZ  4 = PY  4X 2 =cid:82 0.504u2du + 0.5 = 23 .uv1v=0.5u214.4.INDEPENDENCE OF RANDOM VARIABLES1754.4 Independence of random variablesIndependence of events is discussed in Section 2.4. Recall that two events, A and B, are indepen-dent, if and only if P AB = P AP B. Events A, B, and C are mutually independent if theyare pairwise independent, and P ABC = P AP BP C. The general denition of mutual inde-pendence for n random variables was given in Section 2.4.2. Namely, X1, X2, . . . , Xn are mutuallyindependent if any set of events of the form X1  A1,X2  A2, . . . ,Xn  An are mutu-ally independent. In this section we cover in more detail the special case of independence for tworandom variables, although factorization results are given which can be extended to the case of nmutually independent random variables.4.4.1 Denition of independence for two random variablesDenition 4.4.1 Random variables X and Y are dened to be independent if any pair of eventsof the form X  A and Y  B, are independent. That is:4.14Taking A and B to be sets of the form A = u : u  uo and B = v : v  vo shows that if XPX  A, Y  B = PX  APY  B.and Y are independent, the joint CDF factors:4.15Since the CDF completely determines probabilities of the form PX  A, Y  B, it turns outthat that the converse is also true: If the CDF factors i.e. 4.15 holds for all uo, vo. then X andY are independent i.e. 4.14 holds for all A, B.FX,Y uo, vo = FX uoFY vo.In practice, we like to use the generality of 4.14 when doing calculations, but the condition4.15 is easier to check. To illustrate that 4.15 is stronger than it might appear, suppose that4.15 holds for all values of uo, vo, and suppose a < b and c < d. By the four point dierenceformula for CDFs, illustrated in Figure 4.2,Pa < X  b, c < Y  d = FX bFY d  FX bFY c  FX aFY d + FX aFY c= FX b  FX aFY d  FY c = Pa < X  bPc < Y  d.uv110.5v=4u2176CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESTherefore, if 4.15 holds for all values of uo, vo, then 4.14 holds whenever A = a, b and B = c, dfor some a, b, c, d. It can be shown that 4.14 also holds when A and B are nite unions of intervals,and then for all choices of A and B.Recall that for discrete-type random variables it is usually easier to work with pmfs, and forjointly continuous-type random variables it is usually easier to work with pdfs, than with CDFs.Fortunately, in those instances, independence is also equivalent to a factorization property for ajoint pmf or pdf. Therefore, discrete-type random variables X and Y are independent if and onlyif the joint pmf factors:pX,Y u, v = pX upY v,for all u, v. And for jointly continuous-type random variables, X and Y are independent if and onlyif the joint pdf factors:fX,Y u, v = fX ufY v.4.4.2 Determining from a pdf whether independence holdsSuppose X and Y are jointly continuous with joint pdf fX,Y . So X and Y are independent if andonly if fX,Y u.v = fX ufY v for all u and v. It takes a little practice to be able to tell, givena choice of fX,Y , whether independencedenote the length ofthe middle subinterval. If the unit interval represents a stick, and the two points represent locationsof breaks, then X and Y are the lengths of the left and center sticks formed. See Figure 4.11. FindFigure 4.11: Unit interval divided into intervals of length X, Y , and 1  X  Y.the pdf of Y.Solution: We shall rst determine the joint pdf of X and Y , and then integrate it to nd themarginal pdf, fY . Basically we will be using the version of the law of total probability for pdfsin 4.10. The pdf of X is simpleX is uniformly distributed over the interval 0, 1. The supportof fX,Y is the triangular region, T = u, v : u  0, v  0, and u + v  1. In particular,fX,Y u, v = 0 if u  0 or if u  1. For 0 < u < 1, given X = u, the set of possible values of Y isYvf     u|v  ouXf  uf  vuuvvoX|YY|Xf     v|u  ooY01X174CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESthe interval 0, 1  u. Since the second random point is uniformly distributed over u, 1, and Y isthe dierence between that point and the constant u, the conditional distribution of Y is uniformover the interval 0, 1  u. That is, if 0 < u < 1:fY |X v|u =0  v  1  uelse.For 0 < u < 1, fX,Y u, v = fX ufY |X v|u = fX|Y v|u, and fX,Y u, v = 0 otherwise. Therefore,cid:26 11u0cid:26 1cid:90 1vo0cid:12cid:12cid:12cid:121vo0u, v  Telse.The support of the pdf of Y is 0, 1 so let vo  0, 1. ThenfX,Y u, v =1u0cid:90 fY vo =fX,Y u, vodu =11  udu =  ln1  u=  lnvo.This is innite if vo = 0, but the density at a single point doesnt make a dierence, so we setfY 0 = 0. Thus, we have:cid:26  lnv 0 < v < 10else.fY v =Example 4.3.6 Let Z = YX 2 , such that X and Y have joint pdf given bycid:4010fX,Y u, v =if 0  u  1, 0  v  1else.a Find the numerical value of PZ  0.5. Also, sketch a region in the plane so that the area ofthe region is PZ  0.5. b Find the numerical value of PZ  4. Also, sketch a region in theplane so that the area of the region is PZ  4.a PZ  0.5 = PY  0.5X 2 =cid:82 1Solution.0 0.5u2du = 16 .b PZ  4 = PY  4X 2 =cid:82 0.504u2du + 0.5 = 23 .uv1v=0.5u214.4.INDEPENDENCE OF RANDOM VARIABLES1754.4 Independence of random variablesIndependence of events is discussed in Section 2.4. Recall that two events, A and B, are indepen-dent, if and only if P AB = P AP B. Events A, B, and C are mutually independent if theyare pairwise independent, and P ABC = P AP BP C. The general denition of mutual inde-pendence for n random variables was given in Section 2.4.2. Namely, X1, X2, . . . , Xn are mutuallyindependent if any set of events of the form X1  A1,X2  A2, . . . ,Xn  An are mutu-ally independent. In this section we cover in more detail the special case of independence for tworandom variables, although factorization results are given which can be extended to the case of nmutually independent random variables.4.4.1 Denition of independence for two random variablesDenition 4.4.1 Random variables X and Y are dened to be independent if any pair of eventsof the form X  A and Y  B, are independent. That is:4.14Taking A and B to be sets of the form A = u : u  uo and B = v : v  vo shows that if XPX  A, Y  B = PX  APY  B.and Y are independent, the joint CDF factors:4.15Since the CDF completely determines probabilities of the form PX  A, Y  B, it turns outthat that the converse is also true: If the CDF factors i.e. 4.15 holds for all uo, vo. then X andY are independent i.e. 4.14 holds for all A, B.FX,Y uo, vo = FX uoFY vo.In practice, we like to use the generality of 4.14 when doing calculations, but the condition4.15 is easier to check. To illustrate that 4.15 is stronger than it might appear, suppose that4.15 holds for all values of uo, vo, and suppose a < b and c < d. By the four point dierenceformula for CDFs, illustrated in Figure 4.2,Pa < X  b, c < Y  d = FX bFY d  FX bFY c  FX aFY d + FX aFY c= FX b  FX aFY d  FY c = Pa < X  bPc < Y  d.uv110.5v=4u2176CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLESTherefore, if 4.15 holds for all values of uo, vo, then 4.14 holds whenever A = a, b and B = c, dfor some a, b, c, d. It can be shown that 4.14 also holds when A and B are nite unions of intervals,and then for all choices of A and B.Recall that for discrete-type random variables it is usually easier to work with pmfs, and forjointly continuous-type random variables it is usually easier to work with pdfs, than with CDFs.Fortunately, in those instances, independence is also equivalent to a factorization property for ajoint pmf or pdf. Therefore, discrete-type random variables X and Y are independent if and onlyif the joint pmf factors:pX,Y u, v = pX upY v,for all u, v. And for jointly continuous-type random variables, X and Y are independent if and onlyif the joint pdf factors:fX,Y u, v = fX ufY v.4.4.2 Determining from a pdf whether independence holdsSuppose X and Y are jointly continuous with joint pdf fX,Y . So X and Y are independent if andonly if fX,Y u.v = fX ufY v for all u and v. It takes a little practice to be able to tell, givena choice of fX,Y , whether independence