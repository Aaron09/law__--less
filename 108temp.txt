of blackjack!114n = 0.253 orcid:18 0.253114cid:1920.29= 98914.34. The CLT and the Poisson distributiona Consider a Poisson random process Nt : t  0 with rate one. Then X has the samedistribution as N10, which in turn is the sum of 10 independent Poisson random variableswith mean one, because N10 = N1 + N2  N1 +  + N10  N9. Therefore, the CLTsuggests that the distribution of X should be approximately Gaussian.b pX 12 = 1012e1012! = 0.09478.280c Since X has mean and variance equal to , the random variable cid:101X has the N 10, 10cid:111distribution. Therefore, P11.5  cid:101X  12.5 = Pcid:16 2.5cid:17  d fcid:101X 12 == 0.10329. Note: The approximations are o bycid:16 12102cid:110 11.510CHAPTER 6. APPENDIX cid:101X10cid:16 1.5 12.510= 0.103031cid:17cid:17exp210=1010101010210around 10%. This inaccuracy is not too surprising, since ten is a small number ofrandom variables for the CLT based Gaussian approximation.4.36. Conditional means for a joint Gaussian pdf a Since X and X + Y are independent, they are uncorrelated. So0 = CovX, X + Y  = CovX, X + CovX, Y  = VarX + CovX, Y  = 1 + CovX, Y .Therefore, CovX, Y  = 1.b Since X and X + Y are independent, EX|X + Y = 2 = EX = 0.c Since X and Y are jointly Gaussian,EY |X = 2 = cid:98EY |X = 2 = Y + CovY,X2  X  = 2.VarX4.38. Jointly Gaussian Random Variables I18eu12/18.a X has the N 1, 9 distribution; fX u = 1b By the formula for wide sense conditional expectation, cid:98EY |X = 5 = 2+ 6Y  CovX,Y 2cid:179 51 = 4.667,= 16  62and by the formula for the corresponding MSE, 29 = 12.So, the conditional distribution of Y given X = 5 is the N 4.667, 12 distribution;fY |X v|5 = 1c By part b, this is the probability that a random variable with the N 4.667, 12 distri-ev4.6672/24.e = 22X24ubion is greater than or equal to 2, which is Q= Q0.9698 = 0.7793.cid:16 24.66712d The second moment of a random variable Z is EZ2, and it is equal to EZ2 + VarZ.The idea is to apply that observation to the conditional distribution of Y given X = 5.By part b, that conditional distribution is the N 4.667, 12 distribution, so the secondmoment for it is given by EY 2|X = 5 = 4.6672 + 12 = 33.7778.4.40. Estimation of jointly Gaussian random variablesa EZ = EX + 4Y  1 = EX + 4EY   1 = 23.We use the fact CovX, Y  = X Y = 0.445 = 8 to getVarZ = VarX + 4Y  1b Since Z is a linear combination of jointly Gaussian random variables, Z is Gaussian. SoPZ  40 = P= Q= Q0.7759 = 0.2189= VarX + 16  VarY  + 2  4  CovX, Y = 16 + 400 + 64 = 480cid:26 Z  23480cid:27 40  23480cid:18 40  23cid:194806.6. SOLUTIONS TO EVEN NUMBERED PROBLEMS281c Since Z and Y are linear combinations of the jointly Gaussian random variables Xand Y , the variables Z and Y are jointly Gaussian. Therefore, the best unconstrainedestimator of Y given Z is the best linear estimator of Y given Z.SogZ = LZ = cid:98EY |Z = EY  +CovY, ZVarZZ  EZ.UsingCovY, Z = CovY, X + 4Y  1 = CovY, X + 4VarY  = 8 + 100 = 108,we ndandgZ = LZ = 5 +Z  23 = 5 +108480Z  2327120MSE = VarY   CovY, Z2VarZ= 25  1082480= 0.7.4.42. Joint empirical distribution of ECE 313 scoresa By the joint Gaussian assumption,EY |X = u = cid:98EY |X = u = 152 + 350.7119cid:1121  2 = 35u  67 = 152 + 1.31u  67b By the formula for minimum MSE for linear estimation,1  0.712 = 24.65. Thus, given X = u, the conditional distri-e = Ybution of Y is normal with mean 152 + 1.31u  67 and standard deviation 24.65 i.e.variance 607.5.Indexarea rule for expectation, 137average error probability, 63Bayes formula, 53Bernoulli distribution, 37, 240Bernoulli process, 43binomial coecient, 13binomial distribution, 38, 240cardinality, 10Cauchy distribution, 131CDF, see cumulative distribution functioncentral limit theorem, 213Chebychev inequality, 51complementary CDF, 104conditionalexpectation, 167mean, see conditional expectationpdf, 167probability, 32condence interval, 52condence level, 52correlationcoecient, 196count times, 108coupon collector problem, 43cumulative distribution function, 95inverse of, 135joint, 161expectation of a random variable, see mean of arandom variablefailure rate function, 138false alarm probability, 61ow network, 70gamma distribution, 112Gaussianbivariate distribution, 216Gaussian approximation, 119with continuity correction, 120generating a random variable with given distri-bution, 135geometric distribution, 41geometric series, 18increment of a counting process, 44independenceevents, 34pairwise, 35random variables, 36, 175intercount times, 108interval estimator, 52joint probability matrix, 63jointly continuous-type, 165Karnaugh map, 7, 14decision rule, 60DeMorgans laws, 7determinant, 189distribution of a function of a random variable,125Erlang distribution, 112event, 6events, 8Laplace distribution, 144law of large numbers, 211law of the unconscious statof blackjack!114n = 0.253 orcid:18 0.253114cid:1920.29= 98914.34. The CLT and the Poisson distributiona Consider a Poisson random process Nt : t  0 with rate one. Then X has the samedistribution as N10, which in turn is the sum of 10 independent Poisson random variableswith mean one, because N10 = N1 + N2  N1 +  + N10  N9. Therefore, the CLTsuggests that the distribution of X should be approximately Gaussian.b pX 12 = 1012e1012! = 0.09478.280c Since X has mean and variance equal to , the random variable cid:101X has the N 10, 10cid:111distribution. Therefore, P11.5  cid:101X  12.5 = Pcid:16 2.5cid:17  d fcid:101X 12 == 0.10329. Note: The approximations are o bycid:16 12102cid:110 11.510CHAPTER 6. APPENDIX cid:101X10cid:16 1.5 12.510= 0.103031cid:17cid:17exp210=1010101010210around 10%. This inaccuracy is not too surprising, since ten is a small number ofrandom variables for the CLT based Gaussian approximation.4.36. Conditional means for a joint Gaussian pdf a Since X and X + Y are independent, they are uncorrelated. So0 = CovX, X + Y  = CovX, X + CovX, Y  = VarX + CovX, Y  = 1 + CovX, Y .Therefore, CovX, Y  = 1.b Since X and X + Y are independent, EX|X + Y = 2 = EX = 0.c Since X and Y are jointly Gaussian,EY |X = 2 = cid:98EY |X = 2 = Y + CovY,X2  X  = 2.VarX4.38. Jointly Gaussian Random Variables I18eu12/18.a X has the N 1, 9 distribution; fX u = 1b By the formula for wide sense conditional expectation, cid:98EY |X = 5 = 2+ 6Y  CovX,Y 2cid:179 51 = 4.667,= 16  62and by the formula for the corresponding MSE, 29 = 12.So, the conditional distribution of Y given X = 5 is the N 4.667, 12 distribution;fY |X v|5 = 1c By part b, this is the probability that a random variable with the N 4.667, 12 distri-ev4.6672/24.e = 22X24ubion is greater than or equal to 2, which is Q= Q0.9698 = 0.7793.cid:16 24.66712d The second moment of a random variable Z is EZ2, and it is equal to EZ2 + VarZ.The idea is to apply that observation to the conditional distribution of Y given X = 5.By part b, that conditional distribution is the N 4.667, 12 distribution, so the secondmoment for it is given by EY 2|X = 5 = 4.6672 + 12 = 33.7778.4.40. Estimation of jointly Gaussian random variablesa EZ = EX + 4Y  1 = EX + 4EY   1 = 23.We use the fact CovX, Y  = X Y = 0.445 = 8 to getVarZ = VarX + 4Y  1b Since Z is a linear combination of jointly Gaussian random variables, Z is Gaussian. SoPZ  40 = P= Q= Q0.7759 = 0.2189= VarX + 16  VarY  + 2  4  CovX, Y = 16 + 400 + 64 = 480cid:26 Z  23480cid:27 40  23480cid:18 40  23cid:194806.6. SOLUTIONS TO EVEN NUMBERED PROBLEMS281c Since Z and Y are linear combinations of the jointly Gaussian random variables Xand Y , the variables Z and Y are jointly Gaussian. Therefore, the best unconstrainedestimator of Y given Z is the best linear estimator of Y given Z.SogZ = LZ = cid:98EY |Z = EY  +CovY, ZVarZZ  EZ.UsingCovY, Z = CovY, X + 4Y  1 = CovY, X + 4VarY  = 8 + 100 = 108,we ndandgZ = LZ = 5 +Z  23 = 5 +108480Z  2327120MSE = VarY   CovY, Z2VarZ= 25  1082480= 0.7.4.42. Joint empirical distribution of ECE 313 scoresa By the joint Gaussian assumption,EY |X = u = cid:98EY |X = u = 152 + 350.7119cid:1121  2 = 35u  67 = 152 + 1.31u  67b By the formula for minimum MSE for linear estimation,1  0.712 = 24.65. Thus, given X = u, the conditional distri-e = Ybution of Y is normal with mean 152 + 1.31u  67 and standard deviation 24.65 i.e.variance 607.5.Indexarea rule for expectation, 137average error probability, 63Bayes formula, 53Bernoulli distribution, 37, 240Bernoulli process, 43binomial coecient, 13binomial distribution, 38, 240cardinality, 10Cauchy distribution, 131CDF, see cumulative distribution functioncentral limit theorem, 213Chebychev inequality, 51complementary CDF, 104conditionalexpectation, 167mean, see conditional expectationpdf, 167probability, 32condence interval, 52condence level, 52correlationcoecient, 196count times, 108coupon collector problem, 43cumulative distribution function, 95inverse of, 135joint, 161expectation of a random variable, see mean of arandom variablefailure rate function, 138false alarm probability, 61ow network, 70gamma distribution, 112Gaussianbivariate distribution, 216Gaussian approximation, 119with continuity correction, 120generating a random variable with given distri-bution, 135geometric distribution, 41geometric series, 18increment of a counting process, 44independenceevents, 34pairwise, 35random variables, 36, 175intercount times, 108interval estimator, 52joint probability matrix, 63jointly continuous-type, 165Karnaugh map, 7, 14decision rule, 60DeMorgans laws, 7determinant, 189distribution of a function of a random variable,125Erlang distribution, 112event, 6events, 8Laplace distribution, 144law of large numbers, 211law of the unconscious stat