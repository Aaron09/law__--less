ed to include in the sum and correspondingly,the other error probability decreases because there is one fewer entry not underlined to includein the other sum. For example, if we were to modify the sample decision rule above by declaringH0 when X = 1, then pfalse alarm would be reduced from 0.6 to just 0.3 while pmiss would increasefrom 0.0 to 0.1. Whether this pair of error probabilities is better than the original pair depends onconsiderations not modeled here.So far, we have discussed decision rules in general. But which decision rule should be used? Thetwo most widely used methods for coming up with decision rules are described next. Each decisionrule has a corresponding pair pfalse alarm and pmiss. The philosophy we recommend for a given real-world application is to try to evaluate the pair of conditional error probabilities pfalse alarm, pmiss formultiple decision rules and then make a nal selection of decision rule.2.11.1 Maximum likelihood ML decision ruleThe ML decision rule declares the hypothesis which maximizes the probability or likelihood ofthe observation. Operationally, the ML decision rule can be stated as follows: Underline the larger2Use of false alarm is common in the signal processing literature. This is called a type I error in the statisticsliterature. The word miss is used in the signal processing literature for the other type of error, which is called atype II error in the statistics literature.62CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESentry in each column of the likelihood matrix. If the entries in a column of the likelihood matrixare identical, then either can be underlined. The choice may depend on other considerations suchas whether we wish to minimize pfalse alarm or pmiss. The ML rule for the example likelihood matrixabove is the following:X = 0 X = 1 X = 2 X = 3H1H00.00.40.10.30.30.20.60.1 underlines indicatethe ML decision rule .It is easy to check that for the ML decision rule, pfalse alarm = 0.2+0.1 = 0.3 and pmiss = 0.0+0.1 = 0.1.There is another way to express the ML decision rule. Note that for two positive numbers aand b, the statement a > b is equivalent to the statement that ab > 1. Thus, the ML rule can berewritten in a form called a likelihood ratio test LRT as follows. Dene the likelihood ratio kfor each possible observation k as the ratio of the two conditional probabilities:k =p1kp0k.The ML rule is thus equivalent to deciding that H1 is true if X > 1 and deciding H0 is true ifX < 1. The ML rule can be compactly written ascid:26 > 1 declare H1 is true< 1 declare H0 is true.XWe shall see that the other decision rule, described in the next section, can also be expressedas an LRT, but with the threshold 1 changed to dierent values. An LRT with threshold  can bewritten ascid:26 >  declare H1 is trueX<  declare H0 is true.Note that if the threshold  is increased, then there are fewer observations that lead to deciding H1is true. Thus, as  increases, pfalse alarm decreases and pmiss increases. For most binary hypothesistesting problems there is no rule that simultaneously makes both pfalse alarm and pmiss small. In asense, the LRTs are the best possible family of rules, and the parameter  can be used to select agiven operating point on the tradeo between the two error probabilities. As noted above, the MLrule is an LRT with threshold  = 1.2.11.2 Maximum a posteriori probability MAP decision ruleThe other decision rule we discuss requires the computation of joint probabilities such as P X =1  H1. For brevity we write this probability as P H1, X = 1. Such probabilities cannot bededuced from the likelihood matrix alone. Rather, it is necessary for the system designer to assumesome values for P H0 and P H1. Let the assumed value of P Hi be denoted by i, so 0 = P H0and 1 = P H1. The probabilities 0 and 1 are called prior probabilities, because they are theprobabilities assumed prior to when the observation is made.2.11. BINARY HYPOTHESIS TESTING WITH DISCRETE-TYPE OBSERVATIONS63Together the conditional probabilities listed in the likelihood matrix and the prior probabili-ties determine the joint probabilities P Hi, X = k, because P Hi, X = k = ipik. The jointprobability matrix is the matrix of joint probabilities P Hi, X = k. For our rst example, suppose0 = 0.8 and 1 = 0.2. Then the joint probability matrix is given byX = 0 X = 1 X = 2 X = 30.000.120.08.0.320.060.160.020.24H1H0Note that the row for Hi of the joint probability matrix is i times the corresponding row of thelikelihood matrix. Since the row sums for the likelihood matrix are one, the sum for row Hi of thejoint probability matrix is i. Therefore, the sum of all entries in the joint probability matrix isone. The joint probability matrix can be viewed as a Venn diagram.Conditional probabilities such as P H1|X = 2 and P H0|X = 2 are called a posteriori prob-abilities, because they are probabilities that an observer would assign to the two hypotheses aftermaking the observation in this case observing that X = 2. G