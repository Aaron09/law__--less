 with some known distribution. Suppose Y is not observed but thatwe wish to estimate Y . If we use a constant  to estimate Y , the estimation error will be Y  .The mean square error MSE for estimating Y by  is dened by EY  2. By LOTUS, if Y isa continuous-type random variable,cid:90 MSE for estimation of Y by a constant  =y  2fY ydy.4.28We seek to nd  to minimize the MSE. Since Y  2 = Y 2  2Y + 2, we can use linearity ofexpectation to get EY  2 = EY 2  2EY  + 2. This is quadratic in , and the derivativewith resect to  is 2EY  + 2. Therefore the minimum occurs at  = EY . For this value of ,the MSE is EY  2 = VarY . In summary, the constant  that minimizes the mean squareerror for estimation of a random variable Y by a constant is the mean, and the minimum possiblevalue of the mean square error for estimating Y by a constant is VarY .Another way to derive this result is to use the fact that EY  EY  = 0 and EY   is constant,to getEY  2 = EY  EY  + EY  2= EY  EY 2 + 2Y  EY EY   + EY  2= VarY  + EY  2.4.9. MINIMUM MEAN SQUARE ERROR ESTIMATION205From this expression it is easy to see that the mean square error is minimized with respect to  ifand only if  = EY , and the minimum possible value is VarY .4.9.2 Unconstrained estimatorsSuppose instead that we wish to estimate Y based on an observation X. If we use the estimatorgX for some function g, the resulting mean square error MSE is EY  gX2. We want tond g to minimize the MSE. The resulting estimator gX is called the unconstrained optimalestimator of Y based on X because no constraints are placed on the function g.Suppose you observe X = 10. What do you know about Y ? Well, if you know the joint pdfof X and Y , you also know or can derive the conditional pdf of Y given X = 10, denoted byfY |X v|10. Based on the fact, discussed above, that the minimum MSE constant estimator for arandom variable is its mean, it makes sense to estimate Y by the conditional mean:EY |X = 10 =vfY |X v|10dv.cid:90 The resulting conditional MSE is the variance of Y , computed using the conditional distributionof Y given X = 10.EY  EY |X = 102|X = 10 = EY 2|X = 10  EY |X = 102.Conditional expectation indeed gives the optimal estimator, as we show now. Recall thatfX,Y u, v = fX ufY |X v|u. Socid:90 MSE = EY  gX2cid:18cid:90 =v  gu2fY |X v|udvcid:19fX udu.4.29For each u xed, the integral in parentheses in 4.29 has the same form as the integral 4.28.Therefore, for each u, the integral in parentheses in 4.29 is minimized by using gu = gu,where4.304.314.32gu = EY |X = u =We write EY |X for gX. The minimum MSE isM SE = EY  EY |X2cid:90 vfY |X v|udv.cid:19cid:90 cid:90 cid:18cid:90 cid:18cid:90 =a== EY 2  EEY |X2,v  gu2fY |X v|udvv2  gu2fY |X v|udvcid:19fX udufX udu4.33where the equality a follows from the shortcut VarY  = EY 2  EY 2, applied using theconditional distribution of Y given X = u. In summary, the minimum MSE unconstrained estimatorof Y given X is EY |X = gX where gu = EY |X = u, and expressions for the MSE aregiven by 4.31-4.33.206CHAPTER 4. JOINTLY DISTRIBUTED RANDOM VARIABLES4.9.3 Linear estimatorsIn practice it is not always possible to compute gu. Either the integral in 4.30 may not have aclosed form solution, or the conditional density fY |X v|u may not be available or might be dicultto compute. The problems might be more than computational. There might not even be a goodway to decide what joint pdf fX,Y to use in the rst place. A reasonable alternative to using g isto consider linear estimators of Y given X. A linear estimator has the form LX = aX + b, andto specify L we only need to nd the two constants a and b, rather than nding a whole functiong. The MSE for the linear estimator aX + b isM SE = EY  aX + b2.Next we identify the linear estimator that minimizes the MSE. One approach is to multiply outY  aX + b2, take the expectation, and set the derivative with respect to a equal to zero andthe derivative with respect to b equal to zero. That would yield two equations for the unknowns aand b. We will take a slightly dierent approach, rst nding the optimal value of b as a functionof a, substituting that in, and then minimizing over a. The MSE can be written as follows:EY  aX  b2.Therefore, we see that for a given value of a, the constant b should be the minimum MSE constantestimator of Y  aX, which is given by b = EY  aX = Y  aX . Therefore, the optimal linearestimator has the form aX + Y  aX or, equivalently, Y + aX  X , and the correspondingMSE is given byM SE = EY  Y  aX  X 2= VarY  aX= CovY  aX, Y  aX= VarY   2aCovY, X + a2VarX.4.34It remains to nd the constant a. The MSE is quadratic in a, so taking the derivative with respectto a and setting it equal to zero yields that the optimal choice of a is a = CovY,X. Therefore,VarXthe minimum MSE linear estimator is given by LX = cid:98EY |X, wherecid:98EY |X = Y += Y + Y X,Ycid:18 CovY, Xcid:19cid:18 X  XVarXXX  X cid:19.Setting a in 4.34 to a gives the following expression for the minimum possible MSE:minimum MSE for linear es