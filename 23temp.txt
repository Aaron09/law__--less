d the ML estimate of p.Solution: The parameter to be estimated here is p, so p plays the role of  in the denition ofML estimation. We use the letter p in this example instead of  because p is commonlyused in connection with Bernoulli trials. Let X be the number of times heads shows for n ips ofthe coin. It has the binomial distribution with parameters n and p. Therefore, the likelihood thatX = k is pX k =cid:0ncid:1pk1 pnk. Here n is known the number of times the coin is ipped and kis known the number of times heads shows. Therefore, the maximum likelihood estimate,cid:98pM L, isthe value of p that maximizescid:0ncid:1pk1  pnk. Equivalently,cid:98pM L, is the value of p that maximizescid:18 k n  k1  ppk1  pnk. First, assume that 1  k  n  1. Thenpk1  pnk = k  nppk11  pnk1.n . That is, cid:98pM Lk = kpThis derivative is positive if p  kn . Therefore, the likelihood is maximizedat p = kn . A slightly dierent approach to the computation here would be toBut if k = 0 then the likelihood is 1  pn, which is maximized at p = 0, and if k = n then thenote thatcid:98pM L is also the maximum of the log-likelihood: ln pX k. We assumed that 1  k  n 1.likelihood is pn, which is maximized at p = 1, so the formula cid:98pM Lk = kn is true for 0  k  n.n and negative if p  kdpk1  pnkkdpk=cid:192.8. MAXIMUM LIKELIHOOD PARAMETER ESTIMATION49Example 2.8.2 Suppose it is assumed that X is drawn at random from the numbers 1 throughn, with each possibility being equally likely i.e. having probability 1n . Suppose n is unknown butthat it is observed that X = k, for some known value k. Find the ML estimator of n given X = kis observed.Solution: The pmf can be written as pX k = 1n I1kn. Recall that I1kn is the indicatorfunction of 1  k  n, equal to one on that set and equal to zero elsewhere. The whole ideanow is to think of pX k not as a function of k because k is the given observation, but rather,as a function of n. An example is shown in Figure 2.7. It is zero if n  k  1; it jumps up to 1kFigure 2.7: The likelihood of X = k for k xed, as a function of n.at n = k; as n increases beyond k the function decreases. It is thus maximized at n = k. That is,cid:98nM Lk = k.Example 2.8.3 Suppose X has the geometric distribution with some parameter p which is un-known. Suppose a particular value k for X is observed. Find the maximum likelihood estimate,cid:98pM L.pX 1 = p with respect to p, and p = 1 is clearly the maximizer. Socid:98pM L = 1 if k = 1. If k  2 thenSolution: The pmf is given by pX k = 1  pk1p for k  1. If k = 1 we have to maximizepX k = 0 for p = 0 or p = 1. Since 1pk1pcid:48 = k1p1p1pk2 = 1kp1pk2,cid:98pM L = 1we conclude that pX k is increasing in p for 0  p  1k  p  1. Therefore,k if k  2. This expression is correct for k = 1 as well, so for any k  1, cid:98pM L = 1k and decreasing in p for 1k .Example 2.8.4 It is assumed that X has a Poisson distribution with some parameter  with  0, but the value of  is unknown. Suppose it is observed that X = k for a particular integerk. Find the maximum likelihood estimate of X.Solution: The likelihood of observing X = k is ke; the value of  maximizing such likelihoodis to be found for k xed. Equivalently, the value of  maximizing ke is to be found. If k  1,k!dked= k  k1e,n1/k0k50CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESso the likelihood is increasing in  for  < k and decreasing in  for  > k; the likelihood ismaximized at  = k. Thus, cid:98M Lk = k. If k = 0, the likelihood is e, which is maximized by = 0, socid:98M Lk = k for all k  0.2.9 Markov and Chebychev inequalities and condence intervalsThe mean and variance of a random variable are two key numbers that roughly summarize thedistribution of the random variable. In some applications, the mean, and possibly the variance, ofa random variable are known, but the distribution of the random variable is not. The followingtwo inequalities can still be used to provide bounds on the probability a random variable takeson a value far from its mean. The second of these is used to provide a condence interval on anestimator of the parameter p of a binomial distribution.First, the Markov inequality states that if Y is a nonnegative random variable, then for c > 0,PY  c  EY c.To prove Markovs inequality for a discrete-type nonnegative random variable Y with possiblevalues u1, u2, . . . , note that for each i, ui is bounded below by zero if ui < c, and ui is boundedbelow by c if ui  c. Thus,EY  =cid:88 cid:88cid:88i:ui<ci= ci:uicuipY ui0  pY ui +cid:88i:uicpY ui = cPY  c,cpY uiwhich implies the Markov inequality. Equality holds in the Markov inequality if and only if pY 0 +pY c = 1.Example 2.9.1 Suppose 200 balls are distributed among 100 buckets, in some particular butunknown way. For example, all 200 balls could be in the rst bucket, or there could be two ballsin each bucket, or four balls in fty buckets, etc. What is the maximum number of buckets thatcould each have at least ve balls?Solution: This question can be answered without probability thed the ML estimate of p.Solution: The parameter to be estimated here is p, so p plays the role of  in the denition ofML estimation. We use the letter p in this example instead of  because p is commonlyused in connection with Bernoulli trials. Let X be the number of times heads shows for n ips ofthe coin. It has the binomial distribution with parameters n and p. Therefore, the likelihood thatX = k is pX k =cid:0ncid:1pk1 pnk. Here n is known the number of times the coin is ipped and kis known the number of times heads shows. Therefore, the maximum likelihood estimate,cid:98pM L, isthe value of p that maximizescid:0ncid:1pk1  pnk. Equivalently,cid:98pM L, is the value of p that maximizescid:18 k n  k1  ppk1  pnk. First, assume that 1  k  n  1. Thenpk1  pnk = k  nppk11  pnk1.n . That is, cid:98pM Lk = kpThis derivative is positive if p  kn . Therefore, the likelihood is maximizedat p = kn . A slightly dierent approach to the computation here would be toBut if k = 0 then the likelihood is 1  pn, which is maximized at p = 0, and if k = n then thenote thatcid:98pM L is also the maximum of the log-likelihood: ln pX k. We assumed that 1  k  n 1.likelihood is pn, which is maximized at p = 1, so the formula cid:98pM Lk = kn is true for 0  k  n.n and negative if p  kdpk1  pnkkdpk=cid:192.8. MAXIMUM LIKELIHOOD PARAMETER ESTIMATION49Example 2.8.2 Suppose it is assumed that X is drawn at random from the numbers 1 throughn, with each possibility being equally likely i.e. having probability 1n . Suppose n is unknown butthat it is observed that X = k, for some known value k. Find the ML estimator of n given X = kis observed.Solution: The pmf can be written as pX k = 1n I1kn. Recall that I1kn is the indicatorfunction of 1  k  n, equal to one on that set and equal to zero elsewhere. The whole ideanow is to think of pX k not as a function of k because k is the given observation, but rather,as a function of n. An example is shown in Figure 2.7. It is zero if n  k  1; it jumps up to 1kFigure 2.7: The likelihood of X = k for k xed, as a function of n.at n = k; as n increases beyond k the function decreases. It is thus maximized at n = k. That is,cid:98nM Lk = k.Example 2.8.3 Suppose X has the geometric distribution with some parameter p which is un-known. Suppose a particular value k for X is observed. Find the maximum likelihood estimate,cid:98pM L.pX 1 = p with respect to p, and p = 1 is clearly the maximizer. Socid:98pM L = 1 if k = 1. If k  2 thenSolution: The pmf is given by pX k = 1  pk1p for k  1. If k = 1 we have to maximizepX k = 0 for p = 0 or p = 1. Since 1pk1pcid:48 = k1p1p1pk2 = 1kp1pk2,cid:98pM L = 1we conclude that pX k is increasing in p for 0  p  1k  p  1. Therefore,k if k  2. This expression is correct for k = 1 as well, so for any k  1, cid:98pM L = 1k and decreasing in p for 1k .Example 2.8.4 It is assumed that X has a Poisson distribution with some parameter  with  0, but the value of  is unknown. Suppose it is observed that X = k for a particular integerk. Find the maximum likelihood estimate of X.Solution: The likelihood of observing X = k is ke; the value of  maximizing such likelihoodis to be found for k xed. Equivalently, the value of  maximizing ke is to be found. If k  1,k!dked= k  k1e,n1/k0k50CHAPTER 2. DISCRETE-TYPE RANDOM VARIABLESso the likelihood is increasing in  for  < k and decreasing in  for  > k; the likelihood ismaximized at  = k. Thus, cid:98M Lk = k. If k = 0, the likelihood is e, which is maximized by = 0, socid:98M Lk = k for all k  0.2.9 Markov and Chebychev inequalities and condence intervalsThe mean and variance of a random variable are two key numbers that roughly summarize thedistribution of the random variable. In some applications, the mean, and possibly the variance, ofa random variable are known, but the distribution of the random variable is not. The followingtwo inequalities can still be used to provide bounds on the probability a random variable takeson a value far from its mean. The second of these is used to provide a condence interval on anestimator of the parameter p of a binomial distribution.First, the Markov inequality states that if Y is a nonnegative random variable, then for c > 0,PY  c  EY c.To prove Markovs inequality for a discrete-type nonnegative random variable Y with possiblevalues u1, u2, . . . , note that for each i, ui is bounded below by zero if ui < c, and ui is boundedbelow by c if ui  c. Thus,EY  =cid:88 cid:88cid:88i:ui<ci= ci:uicuipY ui0  pY ui +cid:88i:uicpY ui = cPY  c,cpY uiwhich implies the Markov inequality. Equality holds in the Markov inequality if and only if pY 0 +pY c = 1.Example 2.9.1 Suppose 200 balls are distributed among 100 buckets, in some particular butunknown way. For example, all 200 balls could be in the rst bucket, or there could be two ballsin each bucket, or four balls in fty buckets, etc. What is the maximum number of buckets thatcould each have at least ve balls?Solution: This question can be answered without probability the